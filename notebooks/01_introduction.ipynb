{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral-7B Retrieval Augmented Generation (RAG) ‚öôÔ∏è üóÉÔ∏è\n",
    "\n",
    "As the applications of Large Language Models (LLMs) continue to grow, companies and users are increasingly seeking out ways to understand and extract value from their proprietary data by using LLMs. However, security and privacy are serious concerns that have made companies reluctant to expose their sensitive proprietary data to external models. \n",
    "\n",
    "There are two ways this can be addressed. By building LLMs from scratch or fune-tuning open source LLMs on the proprietary data, which can be boht expensive and time consuming. Another option, is to build a RAG framework.\n",
    "\n",
    "Simply put RAG allows users query a data or data source to receive relevant response. \n",
    "RAG frameworks, powered by large language models (LLM), take a data or data source, generate embeddings from the data, store the embeddings in a vector database, perform similarity search on query embeddings across the vector database to find relevant chunks, and then send the query embeddings and relevant chunks to the LLM, which generates a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 17 22:10:32 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA TITAN RTX               Off | 00000000:00:05.0 Off |                  N/A |\n",
      "| 41%   40C    P8              17W / 280W |   9121MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A       941      G   /usr/lib/xorg/Xorg                            8MiB |\n",
      "|    0   N/A  N/A      1122      G   /usr/bin/gnome-shell                          3MiB |\n",
      "|    0   N/A  N/A      1916      C   ollama                                     5630MiB |\n",
      "|    0   N/A  N/A      2162      C   .../Mistral-7B-RAG/Mistral/bin/python3     3472MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import packages\n",
    "\n",
    "- ü¶ô `llama-index` is a framework for fast retrieval and querying of data\n",
    "\n",
    "- üóÑÔ∏è `qdrant` is a vector database and vector similarity search engine for storing, searching and managing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "from llama_index.llms import Ollama\n",
    "import qdrant_client\n",
    "from pathlib import Path\n",
    "from llama_index import download_loader\n",
    "from llama_index import VectorStoreIndex, ServiceContext, SimpleDirectoryReader\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loading the data and Initializing the service context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# reading and loading the data\n",
    "UnstructuredReader = download_loader(\"UnstructuredReader\")\n",
    "loader = UnstructuredReader()\n",
    "docs = loader.load_data(file=Path('../data/data.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to store the data\n",
    "client = qdrant_client.QdrantClient(path=\"../data/qdrant_data\")\n",
    "\n",
    "# name of the collection\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"mistral_data\")\n",
    "\n",
    "# context responsible for storing the nodes, indices, and vectors\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Mistral-7B-RAG/Mistral/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Initializing Ollama and ServiceContext\n",
    "llm = Ollama(model=\"mistral\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local\") # model is located in local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the VectorStoreIndex and query engine\n",
    "index = VectorStoreIndex.from_documents(docs, service_context=service_context, storage_context=storage_context) # embeds data and creates indices for the embeddings\n",
    "query_engine = index.as_query_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a query and stream the response\n",
    "response = query_engine.query(\"what is the story about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The story revolves around Elara, a young woman with mysterious magical abilities in the town of Eldoria. She is known for her enchanting beauty and captivating connection to ancient magic. One day, a stranger named Seraphim arrives in Eldoria seeking her help to unlock a long-lost magic that could save his homeland from a devastating curse. Elara and Seraphim embark on a perilous journey together, encountering various trials and mythical creatures along the way. Their bond transcends boundaries as they forge a new understanding of each other's worlds. The story culminates in a final battle against a malevolent being to break the curse, ultimately freeing Seraphim's homeland while Elara seeks to reconcile her altered identity and find a new purpose for her magic."
     ]
    }
   ],
   "source": [
    "# streaming response\n",
    "response.print_response_stream()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
